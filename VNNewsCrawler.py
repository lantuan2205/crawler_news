import argparse
from concurrent.futures import ThreadPoolExecutor
from logger import log
from utils import utils
from crawler.factory import get_crawler
from utils.ui_checker import UIChecker


def crawl_site(webname, config, ui_checker):
    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl b√°o: {webname}")
    
    # L·∫•y URL c·ªßa trang b√°o t·ª´ config
    base_url = config["news_sites"].get(webname, "")

    #if ui_checker.check_ui_change(base_url):
    #    print(f"UI c·ªßa {webname} ƒë√£ thay ƒë·ªïi! B·ªè qua crawling.")
    #    return

    # T·∫°o m·ªôt b·∫£n config m·ªõi cho t·ª´ng b√°o
    custom_config = config.copy()
    custom_config["webname"] = webname
    custom_config["output_dpath"] = f"{config['output_dpath']}/{webname}"

    # Kh·ªüi t·∫°o crawler v·ªõi config m·ªõi
    crawler = get_crawler(**custom_config)
    crawler.start_crawling()


def main(config_fpath):
    config = utils.get_config(config_fpath)
    log.setup_logging(log_dir=config["output_dpath"], 
                      config_fpath=config["logger_fpath"])
    webnames = config.get("webnames", [])
    # Kh·ªüi t·∫°o UI Checker
    ui_checker = UIChecker(config["ui_hash_file"])

    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y song song
    with ThreadPoolExecutor(max_workers=len(webnames)) as executor:
        futures = []
        for webname in webnames:
            future = executor.submit(crawl_site, webname, config, ui_checker)
            futures.append(future)
        
        # ƒê·ª£i t·∫•t c·∫£ c√°c lu·ªìng ho√†n th√†nh
        for future in futures:
            future.result()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Vietnamese News crawler (with url/type)")
    parser.add_argument("--config", 
                        default="crawler_config.yml", 
                        help="path to config file",
                        dest="config_fpath") 
    args = parser.parse_args()
    main(**vars(args))
